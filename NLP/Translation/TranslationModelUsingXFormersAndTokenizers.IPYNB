{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n",
      "Triton is not available, FusedMLP will not be enabled.\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pathlib import Path\n",
    "from torch import Tensor\n",
    "from torch.nn import Transformer\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt19 (D:/Archives/HuggingfaceCache/datasets/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4387ee0193b4776840600483e7bf865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25984574\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1' # Comment this line if you need to download the dataset from huggingface\n",
    "dataset = load_dataset('wmt19', 'zh-en')\n",
    "print(dataset)\n",
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "SUBSET_SIZE = 50000\n",
    "VOCAB_MIN_FREQ = 10\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "SPECIAL_SYMBOLS = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']\n",
    "VOCAB_PATH = './Model/Vocab.pkl'\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 5\n",
    "NUM_DECODER_LAYERS = 5\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(en) Size: 7884\n",
      "Vocab(zh) Size: 7751\n"
     ]
    }
   ],
   "source": [
    "# Make token transformers that can be used to tokenize text into list of tokens(words).\n",
    "# I use the basic english tokenizer from torchtext for English.\n",
    "# And use jieba for Chinese.\n",
    "token_transform = {}\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer('basic_english')\n",
    "token_transform[SRC_LANGUAGE] = lambda text: ([x for x in jieba.lcut(text) if x not in {' ', '\\t'}])\n",
    "\n",
    "# test_sentence_zh = '但后来他们逐渐意识到所探测到的信号可能完全来源于星际尘埃。'\n",
    "# test_sentence_en = 'It was later realized that the signal they had detected could be entirely attributed to interstellar dust.'\n",
    "# assert token_transform[SRC_LANGUAGE](test_sentence_zh) == ['但', '后来', '他们', '逐渐', '意识', '到', '所', '探测', '到', '的', '信号', '可能', '完全', '来源于', '星际', '尘埃', '。']\n",
    "# assert token_transform[TGT_LANGUAGE](test_sentence_en) == ['it', 'was', 'later', 'realized', 'that', 'the', 'signal', 'they', 'had', 'detected', 'could', 'be', 'entirely', 'attributed', 'to', 'interstellar', 'dust', '.']\n",
    "\n",
    "# Yield tokens from data iterator. For each data {'en':'...', 'zh':'...'} in data_iter, yield a list of tokens in corresponding language using token_transform.\n",
    "def yield_tokens(data_iter, language):\n",
    "    for data in data_iter:\n",
    "        yield token_transform[language](data[language])\n",
    "\n",
    "# Build the vocabulary that can be used to encode token(word) into integer.\n",
    "if Path(VOCAB_PATH).exists():\n",
    "    # If we already have the vocab, load it\n",
    "    with open(VOCAB_PATH, 'rb') as f:\n",
    "        vocab_transform = pickle.load(f)\n",
    "else:\n",
    "    # Otherwise, build the vocab\n",
    "    vocab_transform = {}\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        train_iter = iter(dataset['train'][:SUBSET_SIZE]['translation'])\n",
    "        vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln), min_freq=VOCAB_MIN_FREQ, specials=SPECIAL_SYMBOLS, special_first=True)\n",
    "    for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "        vocab_transform[ln].set_default_index(UNK_IDX)\n",
    "    with open(VOCAB_PATH, 'wb') as f:\n",
    "        pickle.dump(vocab_transform, f)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "print(f'Vocab({TGT_LANGUAGE}) Size: {TGT_VOCAB_SIZE}')\n",
    "print(f'Vocab({SRC_LANGUAGE}) Size: {SRC_VOCAB_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to make the real tokenizer that can tokenize a string of text into a sequence of integer tensors.\n",
    "\n",
    "# Helper function that passes a string into a list of transforms.\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# Helper function that adds a BOS and EOS token to a list of tokens. E.g. [BOS_IDX, 5, 7, ..., 456, EOS_IDX]\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# The real tokenizer.\n",
    "tokenizer = {}\n",
    "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    tokenizer[ln] = sequential_transforms(token_transform[ln], vocab_transform[ln], tensor_transform)\n",
    "\n",
    "# print(tokenizer[TGT_LANGUAGE](test_sentence_en))\n",
    "# Output:\n",
    "# tensor([   2,   17,   38,  660, 3413,   12,    5, 3510,   37,  103,    0,   60, 18, 1667, 4340,    7,    0, 5568,    6,    3])\n",
    "\n",
    "# print(tokenizer[SRC_LANGUAGE](test_sentence_zh))\n",
    "# Output:\n",
    "# tensor([   2,   13, 2221,   36,  843, 1092,   47,   49,    0,   47,    4, 1358, 37,  361, 3490,    0,    0,    6,    3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 3981\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(tokenizer[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(tokenizer[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "class WMT19Dataset(Dataset):\n",
    "    def __init__(self, dataset, subset_size = None):\n",
    "        self.dataset = dataset\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.subset_size is None:\n",
    "            return len(self.dataset)\n",
    "        return self.subset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]['translation'][SRC_LANGUAGE], self.dataset[idx]['translation'][TGT_LANGUAGE]\n",
    "    \n",
    "train_dataset = WMT19Dataset(dataset['train'], SUBSET_SIZE)\n",
    "valid_dataset = WMT19Dataset(dataset['validation'])\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(valid_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 45.21M xFormer(\n",
      "  (rev_enc_pose_encoding): VocabEmbedding(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (position_embeddings): Embedding(256, 512)\n",
      "    (word_embeddings): Embedding(7751, 512)\n",
      "  )\n",
      "  (encoders): ReversibleSequence(\n",
      "    (blocks): ModuleList(\n",
      "      (0-4): 5 x ReversibleBlock(\n",
      "        (f): Deterministic(\n",
      "          (net): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (sublayer): MultiHeadDispatch(\n",
      "              (attention): LinformerAttention(\n",
      "                (E): Linear(in_features=256, out_features=64, bias=False)\n",
      "                (F): Linear(in_features=256, out_features=64, bias=False)\n",
      "                (attn_drop): Dropout(p=0, inplace=False)\n",
      "              )\n",
      "              (in_proj_container): InputProjection(\n",
      "                (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              )\n",
      "              (resid_drop): Dropout(p=0, inplace=False)\n",
      "              (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (g): Deterministic(\n",
      "          (net): PreNorm(\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (sublayer): MLP(\n",
      "              (mlp): Sequential(\n",
      "                (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (1): ReLU()\n",
      "                (2): Dropout(p=0.1, inplace=False)\n",
      "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (4): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoders): ModuleList(\n",
      "    (0): xFormerDecoderBlock(\n",
      "      (pose_encoding): VocabEmbedding(\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (position_embeddings): Embedding(256, 512)\n",
      "        (word_embeddings): Embedding(7884, 512)\n",
      "      )\n",
      "      (wrap_att): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MultiHeadDispatch(\n",
      "            (attention): NystromAttention(\n",
      "              (attn_drop): Dropout(p=0, inplace=False)\n",
      "              (landmark_pooling): AvgPool()\n",
      "            )\n",
      "            (in_proj_container): InputProjection(\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (resid_drop): Dropout(p=0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wrap_cross): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MultiHeadDispatch(\n",
      "            (attention): FavorAttention(\n",
      "              (attn_drop): Dropout(p=0, inplace=True)\n",
      "              (feature_map): SMReg()\n",
      "            )\n",
      "            (in_proj_container): InputProjection(\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (resid_drop): Dropout(p=0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wrap_ff): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MLP(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1-4): 4 x xFormerDecoderBlock(\n",
      "      (wrap_att): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MultiHeadDispatch(\n",
      "            (attention): NystromAttention(\n",
      "              (attn_drop): Dropout(p=0, inplace=False)\n",
      "              (landmark_pooling): AvgPool()\n",
      "            )\n",
      "            (in_proj_container): InputProjection(\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (resid_drop): Dropout(p=0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wrap_cross): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MultiHeadDispatch(\n",
      "            (attention): FavorAttention(\n",
      "              (attn_drop): Dropout(p=0, inplace=True)\n",
      "              (feature_map): SMReg()\n",
      "            )\n",
      "            (in_proj_container): InputProjection(\n",
      "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (resid_drop): Dropout(p=0, inplace=False)\n",
      "            (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (wrap_ff): Residual(\n",
      "        (layer): PreNorm(\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (sublayer): MLP(\n",
      "            (mlp): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (1): ReLU()\n",
      "              (2): Dropout(p=0.1, inplace=False)\n",
      "              (3): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (4): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": True,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": NUM_ENCODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": SRC_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": NUM_DECODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": TGT_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, xformer_config):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.xformers_config = xFormerConfig(xformer_config)\n",
    "        self.xformer = xFormer.from_config(self.xformers_config)\n",
    "        self.generator = nn.Linear(xformer_config[1]['dim_model'], xformer_config[1]['position_encoding_config']['vocab_size'])\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        xformer_out = self.xformer(src, tgt, src_mask, tgt_mask)\n",
    "        return self.generator(xformer_out)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.xformer.encoders(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        return self.xformer.decoders(tgt, memory, tgt_mask)\n",
    "        \n",
    "\n",
    "\n",
    "# This part of xFormers is entirely type checked and needs a config object,\n",
    "# could be changed in the future\n",
    "config = xFormerConfig(model_config)\n",
    "transformer = xFormer.from_config(config)\n",
    "\n",
    "print(f'Model params: {sum(p.numel() for p in transformer.parameters() if p.requires_grad)/1000000:.2f}M', transformer)\n",
    "transformer = transformer.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(transformer.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    src_padding_mask = (src == PAD_IDX)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX)\n",
    "    return src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    total_steps = 0\n",
    "    for src, tgt in tqdm(train_dataloader):\n",
    "        src = src.transpose(0, 1).to(DEVICE)\n",
    "        tgt = tgt.transpose(0, 1).to(DEVICE)\n",
    "        # After transpose, the shape is (BATCH_SIZE, SEQ_LEN)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        logits = model(src, tgt_input, encoder_input_mask=src_padding_mask, decoder_input_mask=tgt_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    total_steps = 0\n",
    "    for src, tgt in valid_dataloader:\n",
    "        src = src.transpose(0, 1).to(DEVICE)\n",
    "        tgt = tgt.transpose(0, 1).to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, encoder_input_mask=src_padding_mask, decoder_input_mask=tgt_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 8.00 GiB total capacity; 6.98 GiB already allocated; 0 bytes free; 7.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x1 \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mrand((BATCH_SIZE, MAX_LEN))\u001b[39m*\u001b[39mSRC_VOCAB_SIZE)\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mint)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m      2\u001b[0m x2 \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mrand((BATCH_SIZE, MAX_LEN))\u001b[39m*\u001b[39mTGT_VOCAB_SIZE)\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mint)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m----> 3\u001b[0m y \u001b[39m=\u001b[39m transformer(src\u001b[39m=\u001b[39;49mx1, tgt\u001b[39m=\u001b[39;49mx2)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\factory\\model_factory.py:301\u001b[0m, in \u001b[0;36mxFormer.forward\u001b[1;34m(self, src, tgt, encoder_input_mask, decoder_input_mask)\u001b[0m\n\u001b[0;32m    298\u001b[0m     tgt \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mclone() \u001b[39mif\u001b[39;00m tgt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m tgt\n\u001b[0;32m    300\u001b[0m     \u001b[39mfor\u001b[39;00m decoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders:\n\u001b[1;32m--> 301\u001b[0m         tgt \u001b[39m=\u001b[39m decoder(\n\u001b[0;32m    302\u001b[0m             target\u001b[39m=\u001b[39;49mtgt,\n\u001b[0;32m    303\u001b[0m             \u001b[39m# pyre-fixme[61]: `memory` is not always initialized here.\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m             memory\u001b[39m=\u001b[39;49mmemory,\n\u001b[0;32m    305\u001b[0m             input_mask\u001b[39m=\u001b[39;49mdecoder_input_mask,\n\u001b[0;32m    306\u001b[0m         )\n\u001b[0;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m tgt\n\u001b[0;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\factory\\block_factory.py:352\u001b[0m, in \u001b[0;36mxFormerDecoderBlock.forward\u001b[1;34m(self, target, memory, encoder_att_mask, decoder_att_mask, input_mask)\u001b[0m\n\u001b[0;32m    347\u001b[0m     target_q, target_k, target_v \u001b[39m=\u001b[39m target, target, target\n\u001b[0;32m    349\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_att(\n\u001b[0;32m    350\u001b[0m     inputs\u001b[39m=\u001b[39m[target_q, target_k, target_v], att_mask\u001b[39m=\u001b[39mdecoder_att_mask\n\u001b[0;32m    351\u001b[0m )\n\u001b[1;32m--> 352\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrap_cross(inputs\u001b[39m=\u001b[39;49m[x, memory, memory], att_mask\u001b[39m=\u001b[39;49mencoder_att_mask)\n\u001b[0;32m    353\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_ff(inputs\u001b[39m=\u001b[39m[x])\n\u001b[0;32m    355\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\residual.py:86\u001b[0m, in \u001b[0;36mResidual.forward\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     residue \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_inputs:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m residue \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer(inputs\u001b[39m=\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m residue \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\residual.py:134\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer(inputs\u001b[39m=\u001b[39minputs_normed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer(\u001b[39m*\u001b[39minputs_normed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\multi_head_dispatch.py:246\u001b[0m, in \u001b[0;36mMultiHeadDispatch.forward\u001b[1;34m(self, query, key, value, att_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    243\u001b[0m     v \u001b[39m=\u001b[39m reshape_fn(v, B, S_K, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_value_head)\n\u001b[0;32m    245\u001b[0m \u001b[39m# Self-attend\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(q, k, v, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw_mask_args)\n\u001b[0;32m    248\u001b[0m \u001b[39m# Re-assemble all head outputs side by side\u001b[39;00m\n\u001b[0;32m    249\u001b[0m y \u001b[39m=\u001b[39m (\n\u001b[0;32m    250\u001b[0m     y\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, S_Q, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_value_head)\n\u001b[0;32m    251\u001b[0m     \u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    252\u001b[0m     \u001b[39m.\u001b[39mflatten(start_dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, end_dim\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m    253\u001b[0m )\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\attention\\favor.py:165\u001b[0m, in \u001b[0;36mFavorAttention.forward\u001b[1;34m(self, q, k, v, *_, **__)\u001b[0m\n\u001b[0;32m    162\u001b[0m     att_raw \u001b[39m=\u001b[39m q_prime \u001b[39m@\u001b[39m (k_prime\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m@\u001b[39m v)\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Actually compute attention\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     att_raw, att_normalization \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_causal_attention(k_prime, q_prime, v)\n\u001b[0;32m    167\u001b[0m \u001b[39m# Normalize\u001b[39;00m\n\u001b[0;32m    168\u001b[0m att \u001b[39m=\u001b[39m att_raw \u001b[39m/\u001b[39m att_normalization\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\attention\\favor.py:125\u001b[0m, in \u001b[0;36mFavorAttention._causal_attention\u001b[1;34m(k_prime, q_prime, v)\u001b[0m\n\u001b[0;32m    123\u001b[0m ref_v \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones_like(v\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m))  \u001b[39m# BATCH x SEQ x 1 x EMB\u001b[39;00m\n\u001b[0;32m    124\u001b[0m Gps \u001b[39m=\u001b[39m k_prime\u001b[39m.\u001b[39munsqueeze(\u001b[39m3\u001b[39m) \u001b[39m*\u001b[39m v\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m Grenorm \u001b[39m=\u001b[39m k_prime\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m3\u001b[39;49m) \u001b[39m*\u001b[39;49m ref_v\n\u001b[0;32m    127\u001b[0m \u001b[39m# Consolidate against the feature dimension\u001b[39;00m\n\u001b[0;32m    128\u001b[0m att_raw \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mbcfe,bcf->bce\u001b[39m\u001b[39m\"\u001b[39m, Gps, q_prime)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 8.00 GiB total capacity; 6.98 GiB already allocated; 0 bytes free; 7.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "x1 = (torch.rand((BATCH_SIZE, MAX_LEN))*SRC_VOCAB_SIZE).abs().to(torch.int).to(DEVICE)\n",
    "x2 = (torch.rand((BATCH_SIZE, MAX_LEN))*TGT_VOCAB_SIZE).abs().to(torch.int).to(DEVICE)\n",
    "y = transformer(src=x1, tgt=x2)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
