{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt19 (D:/Archives/HuggingfaceCache/datasets/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9147bf82e1a74f6b9c48cc940b52db6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25984574\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1' # Comment this line if you need to download the dataset from huggingface\n",
    "dataset = load_dataset('wmt19', 'zh-en')\n",
    "print(dataset)\n",
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "SUBSET_SIZE = 50000\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 5\n",
    "NUM_DECODER_LAYERS = 5\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 512\n",
    "\n",
    "MODEL_SAVE_PATH = './Model/AdvancedTranslationModel.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizers pretrained in Preprocessing/BuildWordPieceTokenizerUsingTokenizersLibrary.IPYNB\n",
    "tokenizer = {SRC_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-zh.json'),\n",
    "             TGT_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-en.json')}\n",
    "SPECIAL_TOKENS = ['[UNK]', '[PAD]', '[BOS]', '[EOS]'] # Don't change this, it's defined in the tokenizer\n",
    "UNK_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[0]) # 0\n",
    "PAD_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[1]) # 1\n",
    "BOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[2]) # 2\n",
    "EOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[3]) # 3\n",
    "SRC_VOCAB_SIZE = tokenizer[SRC_LANGUAGE].get_vocab_size(with_added_tokens=True) # 8623\n",
    "TGT_VOCAB_SIZE = tokenizer[TGT_LANGUAGE].get_vocab_size(with_added_tokens=True) # 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 3981\n"
     ]
    }
   ],
   "source": [
    "class WMT19Dataset(Dataset):\n",
    "    def __init__(self, dataset, subset_size = None):\n",
    "        self.dataset = dataset\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.subset_size is None:\n",
    "            return len(self.dataset)\n",
    "        return self.subset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]['translation'][SRC_LANGUAGE], self.dataset[idx]['translation'][TGT_LANGUAGE]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample.rstrip(\"\\n\"))\n",
    "        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n",
    "\n",
    "    src_batch = torch.tensor([encoded.ids for encoded in tokenizer[SRC_LANGUAGE].encode_batch(src_batch)])\n",
    "    tgt_batch = torch.tensor([encoded.ids for encoded in tokenizer[TGT_LANGUAGE].encode_batch(tgt_batch)])\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "    \n",
    "train_dataset = WMT19Dataset(dataset['train'], SUBSET_SIZE)\n",
    "valid_dataset = WMT19Dataset(dataset['validation'])\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(valid_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# a, b = next(iter(train_dataloader))\n",
    "# print(a, a.shape)\n",
    "# print(b, b.shape)\n",
    "# print(tokenizer[SRC_LANGUAGE].decode_batch(a.tolist()))\n",
    "# print(tokenizer[TGT_LANGUAGE].decode_batch(b.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 50.58 M\n"
     ]
    }
   ],
   "source": [
    "model_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": True,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": NUM_ENCODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": SRC_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": NUM_DECODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": TGT_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, xformer_config):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.xformers_config = xFormerConfig(xformer_config)\n",
    "        self.xformer = xFormer.from_config(self.xformers_config)\n",
    "        self.generator = nn.Linear(xformer_config[1]['dim_model'], xformer_config[1]['position_encoding_config']['vocab_size'])\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        xformer_out = self.xformer(src, tgt, src_mask, tgt_mask)\n",
    "        return self.generator(xformer_out)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        memory = src.clone()\n",
    "        for encoder in self.xformer.encoders:\n",
    "            memory = encoder(memory, input_mask=src_mask)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None):\n",
    "        for decoder in self.xformer.decoders:\n",
    "            tgt = decoder(target=tgt, memory=memory, input_mask=tgt_mask)\n",
    "        return tgt\n",
    "        \n",
    "\n",
    "\n",
    "model = Seq2SeqTransformer(model_config)\n",
    "print(f'Model Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f} M')\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    # Create padding masks, note that a mask value of \"True\" will keep the value\n",
    "    src_padding_mask = (src != PAD_IDX)\n",
    "    tgt_padding_mask = (tgt != PAD_IDX)\n",
    "    return src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    total_steps = 0\n",
    "    for src, tgt in tqdm(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    total_steps = 0\n",
    "    for src, tgt in valid_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Start epoch 1/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ea7a24da0b410a8086c46bd77145b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (131) must match the existing size (113) at non-singleton dimension 0.  Target sizes: [131, -1, -1].  Tensor sizes: [113, 47, 512]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m train_loss \u001b[39m=\u001b[39m train_epoch(model, optimizer)\n\u001b[0;32m      9\u001b[0m end_time \u001b[39m=\u001b[39m timer()\n\u001b[1;32m---> 10\u001b[0m val_loss \u001b[39m=\u001b[39m evaluate(model)\n\u001b[0;32m     11\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), MODEL_SAVE_PATH)\n\u001b[0;32m     12\u001b[0m \u001b[39mprint\u001b[39m((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFinished epoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m| Train loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Val loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Epoch time = \u001b[39m\u001b[39m{\u001b[39;00m(end_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[8], line 36\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     32\u001b[0m tgt_input \u001b[39m=\u001b[39m tgt[:, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     34\u001b[0m src_padding_mask, tgt_padding_mask \u001b[39m=\u001b[39m create_mask(src, tgt_input)\n\u001b[1;32m---> 36\u001b[0m logits \u001b[39m=\u001b[39m model(src, tgt_input, src_mask\u001b[39m=\u001b[39;49msrc_padding_mask, tgt_mask\u001b[39m=\u001b[39;49mtgt_padding_mask)\n\u001b[0;32m     38\u001b[0m tgt_out \u001b[39m=\u001b[39m tgt[:, \u001b[39m1\u001b[39m:]\n\u001b[0;32m     39\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, logits\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), tgt_out\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 81\u001b[0m, in \u001b[0;36mSeq2SeqTransformer.forward\u001b[1;34m(self, src, tgt, src_mask, tgt_mask)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt, src_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, tgt_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 81\u001b[0m     xformer_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxformer(src, tgt, src_mask, tgt_mask)\n\u001b[0;32m     82\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerator(xformer_out)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\factory\\model_factory.py:301\u001b[0m, in \u001b[0;36mxFormer.forward\u001b[1;34m(self, src, tgt, encoder_input_mask, decoder_input_mask)\u001b[0m\n\u001b[0;32m    298\u001b[0m     tgt \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mclone() \u001b[39mif\u001b[39;00m tgt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m tgt\n\u001b[0;32m    300\u001b[0m     \u001b[39mfor\u001b[39;00m decoder \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders:\n\u001b[1;32m--> 301\u001b[0m         tgt \u001b[39m=\u001b[39m decoder(\n\u001b[0;32m    302\u001b[0m             target\u001b[39m=\u001b[39;49mtgt,\n\u001b[0;32m    303\u001b[0m             \u001b[39m# pyre-fixme[61]: `memory` is not always initialized here.\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m             memory\u001b[39m=\u001b[39;49mmemory,\n\u001b[0;32m    305\u001b[0m             input_mask\u001b[39m=\u001b[39;49mdecoder_input_mask,\n\u001b[0;32m    306\u001b[0m         )\n\u001b[0;32m    308\u001b[0m     \u001b[39mreturn\u001b[39;00m tgt\n\u001b[0;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\factory\\block_factory.py:352\u001b[0m, in \u001b[0;36mxFormerDecoderBlock.forward\u001b[1;34m(self, target, memory, encoder_att_mask, decoder_att_mask, input_mask)\u001b[0m\n\u001b[0;32m    347\u001b[0m     target_q, target_k, target_v \u001b[39m=\u001b[39m target, target, target\n\u001b[0;32m    349\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_att(\n\u001b[0;32m    350\u001b[0m     inputs\u001b[39m=\u001b[39m[target_q, target_k, target_v], att_mask\u001b[39m=\u001b[39mdecoder_att_mask\n\u001b[0;32m    351\u001b[0m )\n\u001b[1;32m--> 352\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrap_cross(inputs\u001b[39m=\u001b[39;49m[x, memory, memory], att_mask\u001b[39m=\u001b[39;49mencoder_att_mask)\n\u001b[0;32m    353\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_ff(inputs\u001b[39m=\u001b[39m[x])\n\u001b[0;32m    355\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\residual.py:86\u001b[0m, in \u001b[0;36mResidual.forward\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     residue \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m     85\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_inputs:\n\u001b[1;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m residue \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer(inputs\u001b[39m=\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mreturn\u001b[39;00m residue \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\residual.py:134\u001b[0m, in \u001b[0;36mPreNorm.forward\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer(inputs\u001b[39m=\u001b[39minputs_normed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublayer(\u001b[39m*\u001b[39minputs_normed, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\multi_head_dispatch.py:172\u001b[0m, in \u001b[0;36mMultiHeadDispatch.forward\u001b[1;34m(self, query, key, value, att_mask, key_padding_mask)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m key\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m value\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     max_batch \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], key\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], value\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[1;32m--> 172\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[0;32m    173\u001b[0m         \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mexpand(max_batch, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), [query, key, value]\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    176\u001b[0m B, S_Q, _ \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize()  \u001b[39m# Batch x Sequence x Embedding (latent)\u001b[39;00m\n\u001b[0;32m    177\u001b[0m _, S_K, _ \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39msize()  \u001b[39m# K, Q's sequence length could differ\u001b[39;00m\n",
      "File \u001b[1;32me:\\Software\\Miniconda\\envs\\ml-torch\\lib\\site-packages\\xformers\\components\\multi_head_dispatch.py:173\u001b[0m, in \u001b[0;36mMultiHeadDispatch.forward.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m key\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mor\u001b[39;00m query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m value\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m    171\u001b[0m     max_batch \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m((query\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], key\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], value\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\n\u001b[0;32m    172\u001b[0m     query, key, value \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\n\u001b[1;32m--> 173\u001b[0m         \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39;49mexpand(max_batch, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), [query, key, value]\n\u001b[0;32m    174\u001b[0m     )\n\u001b[0;32m    176\u001b[0m B, S_Q, _ \u001b[39m=\u001b[39m query\u001b[39m.\u001b[39msize()  \u001b[39m# Batch x Sequence x Embedding (latent)\u001b[39;00m\n\u001b[0;32m    177\u001b[0m _, S_K, _ \u001b[39m=\u001b[39m key\u001b[39m.\u001b[39msize()  \u001b[39m# K, Q's sequence length could differ\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (131) must match the existing size (113) at non-singleton dimension 0.  Target sizes: [131, -1, -1].  Tensor sizes: [113, 47, 512]"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = timer()\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Start epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n",
    "    print(\"-\" * 40)\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print((f\"Finished epoch: {epoch + 1}| Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
