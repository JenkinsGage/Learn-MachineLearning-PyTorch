{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1' # Comment this line if you need to download the dataset from huggingface\n",
    "dataset = load_dataset('wmt19', 'zh-en')\n",
    "print(dataset)\n",
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "SUBSET_SIZE = 150000\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0002\n",
    "NUM_EPOCHS = 7\n",
    "SCHEDULER_DECAY_EPOCHS = 4\n",
    "SCHEDULER_GAMMA = 0.5\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # A GPU with memory >=8GB is capable of training\n",
    "\n",
    "EMB_SIZE = 512\n",
    "HIDDEN_LAYER_MULTIPLIER = 2\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 512\n",
    "\n",
    "MODEL_SAVE_PATH = './Model/AdvancedTranslationModel.pth'\n",
    "LOAD_PRETRAINED_MODEL = False # Set it to True if you want to continue with the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizers pretrained in Preprocessing/BuildWordPieceTokenizerUsingTokenizersLibrary.IPYNB\n",
    "tokenizer = {SRC_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-zh.json'),\n",
    "             TGT_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-en.json')}\n",
    "SPECIAL_TOKENS = ['[UNK]', '[PAD]', '[BOS]', '[EOS]'] # Don't change this, it's defined in the tokenizer\n",
    "UNK_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[0]) # 0\n",
    "PAD_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[1]) # 1\n",
    "BOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[2]) # 2\n",
    "EOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[3]) # 3\n",
    "SRC_VOCAB_SIZE = tokenizer[SRC_LANGUAGE].get_vocab_size(with_added_tokens=True)\n",
    "TGT_VOCAB_SIZE = tokenizer[TGT_LANGUAGE].get_vocab_size(with_added_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMT19Dataset(Dataset):\n",
    "    def __init__(self, dataset, subset_size = None):\n",
    "        self.dataset = dataset\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.subset_size is None:\n",
    "            return len(self.dataset)\n",
    "        return self.subset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]['translation'][SRC_LANGUAGE], self.dataset[idx]['translation'][TGT_LANGUAGE]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample.rstrip(\"\\n\"))\n",
    "        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n",
    "\n",
    "    src_batch = torch.tensor([encoded.ids for encoded in tokenizer[SRC_LANGUAGE].encode_batch(src_batch)]) # (Batch, Seq)\n",
    "    tgt_batch = torch.tensor([encoded.ids for encoded in tokenizer[TGT_LANGUAGE].encode_batch(tgt_batch)]) # (Batch, Seq)\n",
    "    \n",
    "    return src_batch, tgt_batch\n",
    "    \n",
    "train_dataset = WMT19Dataset(dataset['train'], SUBSET_SIZE)\n",
    "valid_dataset = WMT19Dataset(dataset['validation'])\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(valid_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# a, b = next(iter(train_dataloader))\n",
    "# print(a, a.shape)\n",
    "# print(b, b.shape)\n",
    "# print(tokenizer[SRC_LANGUAGE].decode_batch(a.tolist()))\n",
    "# print(tokenizer[TGT_LANGUAGE].decode_batch(b.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = [\n",
    "    {\n",
    "        \"reversible\": True,  # Reversible encoder can save a lot memory when training\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": NUM_ENCODER_LAYERS,\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # The vocab type position encoding includes token embedding layer and position encoding layer\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": SRC_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": HIDDEN_LAYER_MULTIPLIER, # Hidden layer dimension is HIDDEN_LAYER_MULTIPLIER times dim_model\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": NUM_DECODER_LAYERS,\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # The vocab type position encoding includes token embedding layer and position encoding layer\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": TGT_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,  # Causal attention is used to prevent the decoder from attending the future tokens in the target sequences\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": HIDDEN_LAYER_MULTIPLIER,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, xformer_config):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.xformers_config = xFormerConfig(xformer_config)\n",
    "        self.xformer = xFormer.from_config(self.xformers_config)\n",
    "        self.generator = nn.Linear(xformer_config[1]['dim_model'], xformer_config[1]['position_encoding_config']['vocab_size'])\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        xformer_out = self.xformer(src, tgt, src_mask, tgt_mask)\n",
    "        return self.generator(xformer_out)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        encoders = self.xformer.encoders\n",
    "        memory = src.clone()\n",
    "        if isinstance(encoders, torch.nn.ModuleList):\n",
    "            for encoder in encoders:\n",
    "                memory = encoder(memory, input_mask=src_mask)\n",
    "        else:\n",
    "            if self.xformer.rev_enc_pose_encoding:\n",
    "                memory = self.xformer.rev_enc_pose_encoding(src)\n",
    "\n",
    "            # Reversible Encoder\n",
    "            x = torch.cat([memory, memory], dim=-1)\n",
    "\n",
    "            # Apply the optional input masking\n",
    "            if src_mask is not None:\n",
    "                if x.dim() - src_mask.dim() > 1:\n",
    "                    src_mask.unsqueeze(0)\n",
    "                x += src_mask.unsqueeze(-1)\n",
    "\n",
    "            x = encoders(x)\n",
    "            memory = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None):\n",
    "        for decoder in self.xformer.decoders:\n",
    "            tgt = decoder(target=tgt, memory=memory, input_mask=tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "model = Seq2SeqTransformer(model_config)\n",
    "print(f'Model Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f} M')\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_DECAY_EPOCHS, gamma=SCHEDULER_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_PRETRAINED_MODEL:\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    # Create padding masks, note that a mask value of \"True\" will keep the value\n",
    "    src_padding_mask = (src != PAD_IDX)\n",
    "    tgt_padding_mask = (tgt != PAD_IDX)\n",
    "    return src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train() # Set model to training mode which enables dropout and batch normalization\n",
    "    losses = 0\n",
    "    \n",
    "    total_steps = 0\n",
    "    for src, tgt in tqdm(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:, :-1] # Tensor tgt has the shape of (Batch, Seq_len), so tgt_input has the shape of (Batch, Seq_len-1) where removed the last [EOS] token\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    total_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in valid_dataloader:\n",
    "            src = src.to(DEVICE)\n",
    "            tgt = tgt.to(DEVICE)\n",
    "\n",
    "            tgt_input = tgt[:, :-1]\n",
    "\n",
    "            src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "            logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "\n",
    "            tgt_out = tgt[:, 1:]\n",
    "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            losses += loss.item()\n",
    "            total_steps += 1\n",
    "    return losses / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = timer()\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Start epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    scheduler.step()\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print((f\"Finished epoch: {epoch + 1}| Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "\n",
    "    # Tensor ys is a temp variable to store the output sequence. It is initialized to the [BOS] token and is then used to generate next token recurrently.\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE) # (Batch, Seq)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(ys, memory) # (Batch, Seq, Dim)\n",
    "        prob = model.generator(out[:, -1, :]) # (Batch, Vocab)\n",
    "        _, next_word = torch.max(prob, dim = 1) # (Batch, )\n",
    "        next_word = next_word.item()\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1) # (Batch, Seq+1)\n",
    "        # Until the predicted next word is [EOS] we stop generating.\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    # Or until it exceeds the max length\n",
    "    return ys\n",
    "\n",
    "def beam_search(model, src, src_mask, max_len, start_symbol, beam_size=3):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "\n",
    "    # Initialize the list of active beams\n",
    "    active_beams = [(ys, 0)]\n",
    "    completed_beams = []\n",
    "\n",
    "    for i in range(max_len-1):\n",
    "        # Store all the candidates of this step\n",
    "        all_candidates = []\n",
    "        for ys, score in active_beams:\n",
    "            out = model.decode(ys, memory)\n",
    "            prob = model.generator(out[:, -1])\n",
    "            # Get the top k probabilities and their corresponding indices\n",
    "            top_prob, top_indices = torch.topk(prob[0], beam_size)\n",
    "            all_candidates.extend([(torch.cat([ys, idx.view(1, 1)], dim=1), score - prob.log()) for idx, prob in zip(top_indices, top_prob)])\n",
    "\n",
    "        # Sort all candidates by score\n",
    "        all_candidates.sort(key=lambda x: x[1])\n",
    "        # Select the top k candidates\n",
    "        active_beams = all_candidates[:beam_size]\n",
    "\n",
    "        # Move the completed beams to a separate list\n",
    "        completed_beams.extend([beam for beam in active_beams if beam[0][0][-1] == EOS_IDX])\n",
    "        active_beams = [beam for beam in active_beams if beam[0][0][-1] != EOS_IDX]\n",
    "\n",
    "        # If there are no more active beams, break\n",
    "        if len(active_beams) == 0:\n",
    "            break\n",
    "\n",
    "    # If there are no completed beams, return the best active beam\n",
    "    if len(completed_beams) == 0:\n",
    "        completed_beams = active_beams\n",
    "\n",
    "    # Sort the completed beams by score and return the best one\n",
    "    completed_beams.sort(key=lambda x: x[1])\n",
    "    return completed_beams[0][0]\n",
    "\n",
    "def translate(model, sentence, use_beam_search=False):\n",
    "    model.eval()\n",
    "    # Encode the input sentence\n",
    "    src = torch.tensor(tokenizer[SRC_LANGUAGE].encode(sentence).ids).view(1, -1) # (Batch, Seq)\n",
    "    # Make mask for the input sentence (useless for single sentence)\n",
    "    src_mask = (src != PAD_IDX)\n",
    "    with torch.no_grad():\n",
    "        if use_beam_search:\n",
    "            translation_tokens = beam_search(model, src, src_mask, MAX_LEN, start_symbol=BOS_IDX).flatten()\n",
    "        else:\n",
    "            translation_tokens = greedy_decode(model, src, src_mask, MAX_LEN, start_symbol=BOS_IDX).flatten()\n",
    "    return tokenizer[TGT_LANGUAGE].decode(translation_tokens.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TEST = 20\n",
    "for i in range(NUM_TEST):\n",
    "    src, truth = valid_dataset[i]\n",
    "    translation = translate(model, src, use_beam_search=True)\n",
    "    print('-'*40)\n",
    "    print(f'Src: {src}')\n",
    "    print(f'Translation: {translation}')\n",
    "    print(f'Truth: {truth}')\n",
    "    print('-'*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
