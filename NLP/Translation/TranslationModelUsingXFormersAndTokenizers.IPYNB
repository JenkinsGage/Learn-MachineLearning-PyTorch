{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled.\n",
      "Error caught was: No module named 'triton'\n",
      "Triton is not available, some optimizations will not be enabled.\n",
      "This is just a warning: No module named 'triton'\n",
      "Triton is not available, FusedMLP will not be enabled.\n",
      "Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt19 (D:/Archives/HuggingfaceCache/datasets/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9147bf82e1a74f6b9c48cc940b52db6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 25984574\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3981\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1' # Comment this line if you need to download the dataset from huggingface\n",
    "dataset = load_dataset('wmt19', 'zh-en')\n",
    "print(dataset)\n",
    "SRC_LANGUAGE = 'zh'\n",
    "TGT_LANGUAGE = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "SUBSET_SIZE = 50000\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 15\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 5\n",
    "NUM_DECODER_LAYERS = 5\n",
    "DROPOUT = 0.1\n",
    "MAX_LEN = 512\n",
    "\n",
    "MODEL_SAVE_PATH = './Model/AdvancedTranslationModel.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizers pretrained in Preprocessing/BuildWordPieceTokenizerUsingTokenizersLibrary.IPYNB\n",
    "tokenizer = {SRC_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-zh.json'),\n",
    "             TGT_LANGUAGE: Tokenizer.from_file('../Preprocessing/Model/tokenizer-wmt19-en.json')}\n",
    "SPECIAL_TOKENS = ['[UNK]', '[PAD]', '[BOS]', '[EOS]'] # Don't change this, it's defined in the tokenizer\n",
    "UNK_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[0]) # 0\n",
    "PAD_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[1]) # 1\n",
    "BOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[2]) # 2\n",
    "EOS_IDX = tokenizer[SRC_LANGUAGE].token_to_id(SPECIAL_TOKENS[3]) # 3\n",
    "SRC_VOCAB_SIZE = tokenizer[SRC_LANGUAGE].get_vocab_size(with_added_tokens=True) # 8623\n",
    "TGT_VOCAB_SIZE = tokenizer[TGT_LANGUAGE].get_vocab_size(with_added_tokens=True) # 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 3981\n"
     ]
    }
   ],
   "source": [
    "class WMT19Dataset(Dataset):\n",
    "    def __init__(self, dataset, subset_size = None):\n",
    "        self.dataset = dataset\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.subset_size is None:\n",
    "            return len(self.dataset)\n",
    "        return self.subset_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]['translation'][SRC_LANGUAGE], self.dataset[idx]['translation'][TGT_LANGUAGE]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample.rstrip(\"\\n\"))\n",
    "        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n",
    "\n",
    "    src_batch = torch.tensor([encoded.ids for encoded in tokenizer[SRC_LANGUAGE].encode_batch(src_batch)])\n",
    "    tgt_batch = torch.tensor([encoded.ids for encoded in tokenizer[TGT_LANGUAGE].encode_batch(tgt_batch)])\n",
    "\n",
    "    return src_batch, tgt_batch\n",
    "    \n",
    "train_dataset = WMT19Dataset(dataset['train'], SUBSET_SIZE)\n",
    "valid_dataset = WMT19Dataset(dataset['validation'])\n",
    "\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Validation dataset size: {len(valid_dataset)}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "# a, b = next(iter(train_dataloader))\n",
    "# print(a, a.shape)\n",
    "# print(b, b.shape)\n",
    "# print(tokenizer[SRC_LANGUAGE].decode_batch(a.tolist()))\n",
    "# print(tokenizer[TGT_LANGUAGE].decode_batch(b.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Params: 50.58 M\n"
     ]
    }
   ],
   "source": [
    "model_config = [\n",
    "    # A list of the encoder or decoder blocks which constitute the Transformer.\n",
    "    # Note that a sequence of different encoder blocks can be used, same for decoders\n",
    "    {\n",
    "        \"reversible\": True,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": NUM_ENCODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": SRC_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,  # Optionally make these layers reversible, to save memory\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": NUM_DECODER_LAYERS,  # Optional, this means that this config will repeat N times\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",  # Optional, pre/post\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"vocab\",  # whatever position encodinhg makes sense\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"vocab_size\": TGT_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",  # whatever attention mechanism\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": 4,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, xformer_config):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.xformers_config = xFormerConfig(xformer_config)\n",
    "        self.xformer = xFormer.from_config(self.xformers_config)\n",
    "        self.generator = nn.Linear(xformer_config[1]['dim_model'], xformer_config[1]['position_encoding_config']['vocab_size'])\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        xformer_out = self.xformer(src, tgt, src_mask, tgt_mask)\n",
    "        return self.generator(xformer_out)\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        memory = src.clone()\n",
    "        for encoder in self.xformer.encoders:\n",
    "            memory = encoder(memory, input_mask=src_mask)\n",
    "        return memory\n",
    "    \n",
    "    def decode(self, tgt, memory, tgt_mask=None):\n",
    "        for decoder in self.xformer.decoders:\n",
    "            tgt = decoder(target=tgt, memory=memory, input_mask=tgt_mask)\n",
    "        return tgt\n",
    "        \n",
    "\n",
    "\n",
    "model = Seq2SeqTransformer(model_config)\n",
    "print(f'Model Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)/1e6:.2f} M')\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src, tgt):\n",
    "    # Create padding masks, note that a mask value of \"True\" will keep the value\n",
    "    src_padding_mask = (src != PAD_IDX)\n",
    "    tgt_padding_mask = (tgt != PAD_IDX)\n",
    "    return src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    \n",
    "    total_steps = 0\n",
    "    for src, tgt in tqdm(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    total_steps = 0\n",
    "    for src, tgt in valid_dataloader:\n",
    "        src = src.transpose(0, 1).to(DEVICE)\n",
    "        tgt = tgt.transpose(0, 1).to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "\n",
    "        src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask=src_padding_mask, tgt_mask=tgt_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        total_steps += 1\n",
    "    return losses / total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Start epoch 1/15\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ea7a24da0b410a8086c46bd77145b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    start_time = timer()\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Start epoch {}/{}\".format(epoch + 1, NUM_EPOCHS))\n",
    "    print(\"-\" * 40)\n",
    "    train_loss = train_epoch(model, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model)\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print((f\"Finished epoch: {epoch + 1}| Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, Epoch time = {(end_time - start_time):.3f}s\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
