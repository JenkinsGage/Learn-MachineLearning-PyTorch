{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8192\n",
    "SPECIAL_TOKENS = ['[UNK]', '[PAD]', '[BOS]', '[EOS]']\n",
    "MIN_FREQUENCY = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer():\n",
    "    # Init the WordPiece based tokenizer with unk_token='[UNK]'\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "    # Setup the normalization pipeline\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(single='[BOS] $A [EOS]', special_tokens=[('[BOS]', 2), ('[EOS]', 3)])\n",
    "    tokenizer.enable_padding(pad_id=1, pad_token='[PAD]')\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "    return tokenizer\n",
    "\n",
    "# Setup the trainer with specific vocab_size, special_tokens and min_frequency\n",
    "trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS, min_frequency=MIN_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can train the tokenizer from wikitext file\n",
    "## wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "## unzip wikitext-103-raw-v1.zip\n",
    "tokenizer_train_files = [f'./Data/wikitext-103-raw-v1/wikitext-103-raw/wiki.{split}.raw' for split in ['train', 'valid', 'test']]\n",
    "\n",
    "tokenizer_wiki = init_tokenizer()\n",
    "tokenizer_wiki.train(tokenizer_train_files, trainer)\n",
    "\n",
    "tokenizer_wiki.save('./Model/tokenizer-wiki.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from D:\\Archives\\HuggingfaceCache\\modules\\datasets_modules\\datasets\\wmt19\\29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a (last modified on Sat Apr  1 21:51:16 2023) since it couldn't be found locally at wmt19., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset wmt19 (D:/Archives/HuggingfaceCache/datasets/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9766ad194974b07be336fbdc965c1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Or train from iterator\n",
    "# Let's load the dataset first\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "dataset = load_dataset('wmt19', 'zh-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an interator to get a batch of sentences from the dataset\n",
    "def batch_iterator(batch_size=1000, language='en', subset_rows=250000):\n",
    "    for split in ['train', 'validation']:\n",
    "        for i in range(0, dataset.num_rows[split] if split == 'validation' else subset_rows, batch_size):\n",
    "            batch = dataset[split][i:i + batch_size]['translation']\n",
    "            batch = [d[language] for d in batch]\n",
    "            yield batch\n",
    "\n",
    "# print(next(batch_iterator(2)))\n",
    "# Output: ['1929 or 1989?', 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.']\n",
    "\n",
    "tokenizer_wmt19_en = init_tokenizer()\n",
    "tokenizer_wmt19_en.train_from_iterator(batch_iterator(language='en'), trainer=trainer)\n",
    "tokenizer_wmt19_en.save('./Model/tokenizer-wmt19-en.json')\n",
    "\n",
    "tokenizer_wmt19_zh = init_tokenizer()\n",
    "tokenizer_wmt19_zh.train_from_iterator(batch_iterator(language='zh'), trainer=trainer)\n",
    "tokenizer_wmt19_zh.save('./Model/tokenizer-wmt19-zh.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'when', 'en', '##c', '##od', '##ing', 'm', '##ult', '##ip', '##le', 'se', '##n', '##ten', '##ces', ',', 'you', 'can', 'aut', '##om', '##at', '##ically', 'p', '##ad', 'the', 'out', '##p', '##ut', '##s', 'to', 'the', 'long', '##est', 'se', '##n', '##ten', '##ce', 'pres', '##ent', 'by', 'us', '##ing', 'to', '##ke', '##n', '##iz', '##er', '.', '[EOS]']\n",
      "[2, 7418, 7254, 4063, 7206, 7123, 54, 7400, 7259, 7150, 7203, 4062, 7706, 7476, 15, 7988, 7427, 7915, 7132, 7114, 8032, 57, 7144, 7107, 7404, 4082, 7162, 4075, 7127, 7107, 7708, 7196, 7203, 4062, 7706, 7185, 7583, 7137, 7179, 7336, 7123, 7127, 7414, 4062, 7384, 7108, 17, 3]\n",
      "when encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using tokenizer.\n"
     ]
    }
   ],
   "source": [
    "tokenizer_wiki = Tokenizer.from_file('Model/tokenizer-wiki.json')\n",
    "\n",
    "output = tokenizer_wiki.encode(\"When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using Tokenizer.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(tokenizer_wiki.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'it', 'was', 'later', 'realized', 'that', 'the', 'signal', 'they', 'had', 'det', '##ected', 'could', 'be', 'entirely', 'attrib', '##uted', 'to', 'inter', '##st', '##ell', '##ar', 'd', '##ust', '.', '[EOS]']\n",
      "[2, 234, 362, 2169, 6641, 226, 189, 6150, 364, 662, 1132, 979, 545, 235, 3637, 5322, 2165, 205, 359, 220, 465, 203, 42, 311, 17, 3]\n",
      "['[BOS]', '但', '##后', '##来', '##他', '##们', '##逐', '##渐', '##意', '##识', '##到', '##所', '##探', '##测', '##到', '##的', '##信', '##号', '##可', '##能', '##完', '##全', '##来', '##源', '##于', '##星', '##际', '##尘', '##埃', '。', '[EOS]']\n",
      "[2, 307, 5030, 4950, 4923, 4824, 5571, 5119, 4849, 4825, 4743, 4642, 6103, 5114, 4743, 4650, 4813, 4669, 4731, 4740, 4832, 4827, 4950, 4945, 4834, 5826, 4944, 7169, 5224, 147, 3]\n",
      "it was later realized that the signal they had detected could be entirely attributed to interstellar dust.\n",
      "但后来他们逐渐意识到所探测到的信号可能完全来源于星际尘埃 。\n"
     ]
    }
   ],
   "source": [
    "tokenizer_wmt19_en = Tokenizer.from_file('./Model/tokenizer-wmt19-en.json')\n",
    "tokenizer_wmt19_zh = Tokenizer.from_file('./Model/tokenizer-wmt19-zh.json')\n",
    "\n",
    "test_sentence_en = 'It was later realized that the signal they had detected could be entirely attributed to interstellar dust.'\n",
    "test_sentence_zh = '但后来他们逐渐意识到所探测到的信号可能完全来源于星际尘埃。'\n",
    "\n",
    "encoded_en = tokenizer_wmt19_en.encode(test_sentence_en)\n",
    "encoded_zh = tokenizer_wmt19_zh.encode(test_sentence_zh)\n",
    "\n",
    "print(encoded_en.tokens)\n",
    "print(encoded_en.ids)\n",
    "\n",
    "print(encoded_zh.tokens)\n",
    "print(encoded_zh.ids)\n",
    "\n",
    "print(tokenizer_wmt19_en.decode(encoded_en.ids))\n",
    "print(tokenizer_wmt19_zh.decode(encoded_zh.ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
