{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "SPECIAL_TOKENS = ['[UNK]', '[PAD]', '[BOS]', '[EOS]']\n",
    "MIN_FREQUENCY = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer():\n",
    "    # Init the WordPiece based tokenizer with unk_token='[UNK]'\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token='[UNK]'))\n",
    "    # Setup the normalization pipeline\n",
    "    tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(single='[BOS] $A [EOS]', special_tokens=[('[BOS]', 2), ('[EOS]', 3)])\n",
    "    tokenizer.enable_padding(pad_id=1, pad_token='[PAD]')\n",
    "    tokenizer.decoder = decoders.WordPiece()\n",
    "    return tokenizer\n",
    "\n",
    "# Setup the trainer with specific vocab_size, special_tokens and min_frequency\n",
    "trainer = WordPieceTrainer(vocab_size=VOCAB_SIZE, special_tokens=SPECIAL_TOKENS, min_frequency=MIN_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can train the tokenizer from wikitext file\n",
    "## wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-raw-v1.zip\n",
    "## unzip wikitext-103-raw-v1.zip\n",
    "tokenizer_train_files = [f'./Data/wikitext-103-raw-v1/wikitext-103-raw/wiki.{split}.raw' for split in ['train', 'valid', 'test']]\n",
    "\n",
    "tokenizer_wiki = init_tokenizer()\n",
    "tokenizer_wiki.train(tokenizer_train_files, trainer)\n",
    "\n",
    "tokenizer_wiki.save('./Model/tokenizer-wiki.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wmt19 (D:/Archives/HuggingfaceCache/datasets/wmt19/zh-en/1.0.0/29e210fae5690e843cae5dc43b53db36c4e02f927db50cd5235a22ab42dde90a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8094119d9234708800ad07d4292419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Or train from iter\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "# Load translation dataset from huggingface\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1' # Comment this line if you need to download the dataset from huggingface\n",
    "dataset = load_dataset('wmt19', 'zh-en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an interator to get a batch of sentences from the dataset\n",
    "def batch_iterator(batch_size=1000, language='en', subset_rows=150000):\n",
    "    for split in ['train', 'validation']:\n",
    "        for i in range(0, dataset.num_rows[split] if split == 'validation' else subset_rows, batch_size):\n",
    "            batch = dataset[split][i:i + batch_size]['translation']\n",
    "            batch = [d[language] for d in batch]\n",
    "            yield batch\n",
    "\n",
    "# print(next(batch_iterator(2)))\n",
    "# Output: ['1929 or 1989?', 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.']\n",
    "\n",
    "tokenizer_wmt19_en = init_tokenizer()\n",
    "tokenizer_wmt19_en.train_from_iterator(batch_iterator(language='en'), trainer=trainer)\n",
    "tokenizer_wmt19_en.save('./Model/tokenizer-wmt19-en.json')\n",
    "\n",
    "tokenizer_wmt19_zh = init_tokenizer()\n",
    "tokenizer_wmt19_zh.train_from_iterator(batch_iterator(language='zh'), trainer=trainer)\n",
    "tokenizer_wmt19_zh.save('./Model/tokenizer-wmt19-zh.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_wiki = Tokenizer.from_file('Model/tokenizer-wiki.json')\n",
    "\n",
    "output = tokenizer_wiki.encode(\"When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by using Tokenizer.\")\n",
    "print(output.tokens)\n",
    "print(output.ids)\n",
    "print(tokenizer_wiki.decode(output.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[BOS]', 'it', 'was', 'later', 'realized', 'that', 'the', 'signal', 'they', 'had', 'detect', '##ed', 'could', 'be', 'entirely', 'attrib', '##uted', 'to', 'inter', '##st', '##ell', '##ar', 'd', '##ust', '.', '[EOS]']\n",
      "[2, 225, 362, 2217, 6575, 217, 180, 6243, 359, 657, 7738, 191, 530, 226, 3516, 5227, 2145, 196, 349, 211, 463, 195, 42, 300, 17, 3]\n",
      "['[BOS]', '但', '##后', '##来', '##他', '##们', '##逐', '##渐', '##意', '##识', '##到', '##所', '##探', '##测', '##到', '##的', '##信', '##号', '##可', '##能', '##完', '##全', '##来', '##源', '##于', '##星', '##际', '##尘', '##埃', '。', '[EOS]']\n",
      "[2, 254, 4823, 4524, 4497, 4681, 5236, 5291, 4499, 5281, 4472, 4452, 6205, 5486, 4472, 4369, 4684, 5116, 4603, 4384, 5102, 4624, 4524, 5286, 4637, 5902, 4511, 6126, 5672, 98, 3]\n",
      "it was later realized that the signal they had detected could be entirely attributed to interstellar dust.\n",
      "但后来他们逐渐意识到所探测到的信号可能完全来源于星际尘埃 。\n"
     ]
    }
   ],
   "source": [
    "tokenizer_wmt19_en = Tokenizer.from_file('./Model/tokenizer-wmt19-en.json')\n",
    "tokenizer_wmt19_zh = Tokenizer.from_file('./Model/tokenizer-wmt19-zh.json')\n",
    "\n",
    "test_sentence_en = 'It was later realized that the signal they had detected could be entirely attributed to interstellar dust.'\n",
    "test_sentence_zh = '但后来他们逐渐意识到所探测到的信号可能完全来源于星际尘埃。'\n",
    "\n",
    "encoded_en = tokenizer_wmt19_en.encode(test_sentence_en)\n",
    "encoded_zh = tokenizer_wmt19_zh.encode(test_sentence_zh)\n",
    "\n",
    "print(encoded_en.tokens)\n",
    "print(encoded_en.ids)\n",
    "\n",
    "print(encoded_zh.tokens)\n",
    "print(encoded_zh.ids)\n",
    "\n",
    "print(tokenizer_wmt19_en.decode(encoded_en.ids))\n",
    "print(tokenizer_wmt19_zh.decode(encoded_zh.ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
