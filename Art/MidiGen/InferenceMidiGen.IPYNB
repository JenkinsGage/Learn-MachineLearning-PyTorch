{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import ast\n",
    "from dataclasses import dataclass\n",
    "from xformers.factory.model_factory import xFormer, xFormerConfig\n",
    "from xformers.components.positional_embedding import (PositionEmbedding, PositionEmbeddingConfig, register_positional_embedding)\n",
    "\n",
    "import pretty_midi\n",
    "import librosa\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os\n",
    "import mir_eval.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init model from config\n",
    "\n",
    "PAD_IDX = 128\n",
    "BOS_IDX = 129\n",
    "EOS_IDX = 130\n",
    "PAD_VALUE = 0.0\n",
    "\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "EMB_SIZE=64\n",
    "MAX_LEN = 256\n",
    "SRC_VOCAB_SIZE = 128+3 # 0-127 representing From C-1 to G9, 128 for PAD_IDX, 129 for BOS, 130 for EOS\n",
    "TGT_VOCAB_SIZE = 128+3\n",
    "NHEAD = 4\n",
    "HIDDEN_LAYER_MULTIPLIER = 4\n",
    "DROPOUT = 0.2\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_SAVE_PATH = './Model/MidiGen.pth'\n",
    "\n",
    "@dataclass\n",
    "class MidiEmbeddingConfig(PositionEmbeddingConfig):\n",
    "    pitch_size: int\n",
    "    dropout: float\n",
    "\n",
    "\n",
    "@register_positional_embedding(\"midi\", MidiEmbeddingConfig)\n",
    "class MidiEmbedding(PositionEmbedding):\n",
    "    def __init__(self, dim_model: int, seq_len: int, pitch_size: int, dropout: float = 0.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.dim_model = dim_model\n",
    "        self.seq_len = seq_len\n",
    "        self.pitch_size = pitch_size\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        self.position_embeddings = nn.Embedding(seq_len, self.dim_model)\n",
    "        self.pitch_embeddings = nn.Embedding(\n",
    "            self.pitch_size, self.dim_model - 3)\n",
    "\n",
    "        self.position_ids: Optional[torch.Tensor] = None\n",
    "\n",
    "    def init_weights(self, gain: float = 1.0):\n",
    "        torch.nn.init.normal_(self.position_embeddings.weight, std=0.02 * gain)\n",
    "        torch.nn.init.normal_(self.pitch_embeddings.weight, std=0.02 * gain)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        sentence = x[0]\n",
    "        extra = x[1]\n",
    "\n",
    "        position_ids = torch.arange(sentence.shape[1], dtype=torch.long, device=sentence.device)[\n",
    "            None, :\n",
    "        ].repeat(sentence.shape[0], 1)\n",
    "\n",
    "        pitch_token = self.pitch_embeddings(sentence)\n",
    "\n",
    "        x = torch.cat([pitch_token, extra], dim=-1)\n",
    "        pos = self.position_embeddings(position_ids)\n",
    "\n",
    "        X = x + pos\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "\n",
    "class MidiTransformer(nn.Module):\n",
    "    def __init__(self, model_config) -> None:\n",
    "        super().__init__()\n",
    "        self.dim_model = model_config[1]['dim_model']\n",
    "        self.model_config = xFormerConfig(model_config)\n",
    "        self.transformer = xFormer.from_config(self.model_config)\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.dim_model, self.dim_model*2), \n",
    "            nn.LeakyReLU(), \n",
    "            nn.Linear(self.dim_model*2, model_config[1]['position_encoding_config']['pitch_size']))\n",
    "        self.extra_generator = nn.Sequential(\n",
    "            nn.Linear(self.dim_model, self.dim_model*2), \n",
    "            nn.Linear(self.dim_model*2, 3))  # [time_since_last_note, duration, velocity]\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        out = self.decode(tgt, memory, tgt_mask)\n",
    "        return self.generator(out), self.extra_generator(out)\n",
    "\n",
    "    def encode(self, src, src_mask=None):\n",
    "        encoders = self.transformer.encoders\n",
    "        memory = src[:]\n",
    "        if isinstance(encoders, torch.nn.ModuleList):\n",
    "            for encoder in encoders:\n",
    "                memory = encoder(memory, input_mask=src_mask)\n",
    "        else:\n",
    "            if self.transformer.rev_enc_pose_encoding:\n",
    "                memory = self.transformer.rev_enc_pose_encoding(src)\n",
    "\n",
    "            # Reversible Encoder\n",
    "            x = torch.cat([memory, memory], dim=-1)\n",
    "\n",
    "            # Apply the optional input masking\n",
    "            if src_mask is not None:\n",
    "                if x.dim() - src_mask.dim() > 1:\n",
    "                    src_mask.unsqueeze(0)\n",
    "                x += src_mask.unsqueeze(-1)\n",
    "\n",
    "            x = encoders(x)\n",
    "            memory = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n",
    "        return memory\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask=None):\n",
    "        for decoder in self.transformer.decoders:\n",
    "            tgt = decoder(target=tgt, memory=memory, input_mask=tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "model_config = [\n",
    "    {\n",
    "        \"reversible\": True,  # Reversible encoder can save a lot memory when training\n",
    "        \"block_type\": \"encoder\",\n",
    "        \"num_layers\": NUM_ENCODER_LAYERS,\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"midi\",  # The vocab type position encoding includes token embedding layer and position encoding layer\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"pitch_size\": SRC_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"linformer\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            # Hidden layer dimension is HIDDEN_LAYER_MULTIPLIER times dim_model\n",
    "            \"hidden_layer_multiplier\": HIDDEN_LAYER_MULTIPLIER,\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"reversible\": False,\n",
    "        \"block_type\": \"decoder\",\n",
    "        \"num_layers\": NUM_DECODER_LAYERS,\n",
    "        \"dim_model\": EMB_SIZE,\n",
    "        \"residual_norm_style\": \"pre\",\n",
    "        \"position_encoding_config\": {\n",
    "            \"name\": \"midi\",\n",
    "            \"seq_len\": MAX_LEN,\n",
    "            \"pitch_size\": TGT_VOCAB_SIZE,\n",
    "        },\n",
    "        \"multi_head_config_masked\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"nystrom\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": True,  # Causal attention is used to prevent the decoder from attending the future tokens in the target sequences\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"multi_head_config_cross\": {\n",
    "            \"num_heads\": NHEAD,\n",
    "            \"residual_dropout\": 0,\n",
    "            \"attention\": {\n",
    "                \"name\": \"favor\",\n",
    "                \"dropout\": 0,\n",
    "                \"causal\": False,\n",
    "                \"seq_len\": MAX_LEN,\n",
    "            },\n",
    "        },\n",
    "        \"feedforward_config\": {\n",
    "            \"name\": \"MLP\",\n",
    "            \"dropout\": DROPOUT,\n",
    "            \"activation\": \"relu\",\n",
    "            \"hidden_layer_multiplier\": HIDDEN_LAYER_MULTIPLIER,\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "model = MidiTransformer(model_config=model_config)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model weights\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask=None, max_len=MAX_LEN, start_symbol=BOS_IDX):\n",
    "    src = (src[0].to(DEVICE), src[1].to(DEVICE))\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys_token = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    ys_extra = torch.ones(1, 1, 3).fill_(PAD_VALUE).type(torch.long).to(DEVICE)\n",
    "    ys = (ys_token, ys_extra)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(ys, memory)\n",
    "        prob = model.generator(out[:, -1, :])\n",
    "        extra = model.extra_generator(out[:, -1, :]).unsqueeze(0)\n",
    "        _, next_token = torch.max(prob, dim=1)\n",
    "        next_token = next_token.item()\n",
    "        ys_token = torch.cat([ys_token, torch.ones(1, 1).type_as(src[0].data).fill_(next_token)], dim=1)\n",
    "        ys_extra = torch.cat([ys_extra, extra], dim=1)\n",
    "        ys = (ys_token, ys_extra)\n",
    "        if next_token == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "def predict_next_midi_sentence(model, src):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        result = greedy_decode(model, src)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_piano_roll(pm:pretty_midi.PrettyMIDI, start_pitch=pretty_midi.note_name_to_number('C2'), end_pitch=pretty_midi.note_name_to_number('G7'), fs=100):\n",
    "    \"\"\"\n",
    "    Plot a piano roll of a pretty_midi object.\n",
    "\n",
    "    Args:\n",
    "        pm (pretty_midi.PrettyMIDI): A pretty_midi object.\n",
    "        start_pitch (int): The MIDI pitch number to start plotting from.\n",
    "        end_pitch (int): The MIDI pitch number to end plotting at.\n",
    "        fs (int, optional): The sampling frequency used to generate the piano roll. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Use librosa's specshow function for displaying the piano roll\n",
    "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
    "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
    "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
    "    \n",
    "def play_pm(pm:pretty_midi.PrettyMIDI, fs=44100) -> None:\n",
    "    display(IPython.display.Audio(pm.synthesize(fs=fs), rate=fs))\n",
    "\n",
    "\n",
    "def print_midi_info(pm:pretty_midi.PrettyMIDI):\n",
    "    print(f'There are {len(pm.time_signature_changes)} time signature changes')\n",
    "    print(f'There are {len(pm.instruments)} instruments')\n",
    "    print(f'Tempo: {pm.estimate_tempo()}')\n",
    "    for i, instrument in enumerate(pm.instruments):\n",
    "        print('-'*40)\n",
    "        print(f'Instrument {i} has {len(instrument.notes)} notes')\n",
    "        notes_info = []\n",
    "        for note in instrument.notes:\n",
    "            note_info = {\n",
    "                'Tick': pm.time_to_tick(note.start),\n",
    "                'StartTime': note.start,\n",
    "                'EndTime': note.end,\n",
    "                'Pitch': note.pitch,\n",
    "                'Note': pretty_midi.note_number_to_name(note.pitch),\n",
    "                'Velocity': note.velocity,\n",
    "                'Duration': note.end - note.start\n",
    "            }\n",
    "            notes_info.append(note_info)\n",
    "        df = pd.DataFrame(notes_info)\n",
    "        df.sort_values('Tick', inplace=True)\n",
    "        print(df)\n",
    "    print('-'*40)\n",
    "\n",
    "def plot_piano_roll_with_beats(pm, start_pitch=pretty_midi.note_name_to_number('C2'), end_pitch=pretty_midi.note_name_to_number('G7'), fs=100, xlim=(0, 5)):\n",
    "    \"\"\"\n",
    "    Plot a piano roll of a pretty_midi object, with beat and downbeat markers.\n",
    "\n",
    "    Args:\n",
    "        pm (pretty_midi.PrettyMIDI): A pretty_midi object.\n",
    "        start_pitch (int): The MIDI pitch number to start plotting from.\n",
    "        end_pitch (int): The MIDI pitch number to end plotting at.\n",
    "        fs (int, optional): The sampling frequency used to generate the piano roll. Defaults to 100.\n",
    "        xlim (int, optional): The x-limit of the plot. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Get beat and downbeat times\n",
    "    beats = pm.get_beats()\n",
    "    downbeats = pm.get_downbeats()\n",
    "\n",
    "    # Plot piano roll\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plot_piano_roll(pm, start_pitch, end_pitch, fs=fs)\n",
    "    ymin, ymax = plt.ylim()\n",
    "\n",
    "    # Plot beats as grey lines, downbeats as red lines\n",
    "    mir_eval.display.events(beats, base=ymin, height=ymax, color='#AAAAAA')\n",
    "    mir_eval.display.events(downbeats, base=ymin, height=ymax, color='r')\n",
    "\n",
    "    # Set xlim for clarity\n",
    "    plt.xlim(xlim)\n",
    "    plt.show()\n",
    "\n",
    "def quantize_pretty_midi(pm:pretty_midi.PrettyMIDI, threshold=1/8):\n",
    "    quantized_pm = pretty_midi.PrettyMIDI()\n",
    "    beats = pm.get_beats()\n",
    "    threshold = threshold * (beats[1]- beats[0])\n",
    "    for instrument in pm.instruments:\n",
    "        quantized_instrument = pretty_midi.Instrument(program=instrument.program)\n",
    "        notes = []\n",
    "        for note in instrument.notes:\n",
    "            nearest_start_beat = min(beats, key=lambda x: abs(x - note.start))\n",
    "            if abs(nearest_start_beat - note.start) <= threshold:\n",
    "                note.start = nearest_start_beat\n",
    "            if abs(note.end - note.start) < threshold:\n",
    "                nearest_end_beat = min(beats, key=lambda x: abs(x - note.end))\n",
    "                if abs(nearest_end_beat - note.end) <= threshold:\n",
    "                    note.end = nearest_end_beat\n",
    "            notes.append(note)\n",
    "        notes.sort(key=lambda note: note.start)\n",
    "        quantized_instrument.notes = notes\n",
    "        quantized_pm.instruments.append(instrument)\n",
    "\n",
    "    return quantized_pm\n",
    "\n",
    "def pm_to_df(pm: pretty_midi.PrettyMIDI) -> pd.DataFrame:\n",
    "    # Get the downbeats\n",
    "    downbeats = pm.get_downbeats()\n",
    "\n",
    "    # Initialize lists for each column\n",
    "    sentence_list = []\n",
    "    time_since_downbeat_list = []\n",
    "    duration_list = []\n",
    "    velocity_list = []\n",
    "    time_since_last_note_start_list = []\n",
    "    instrument_program_list = []\n",
    "    track_list = []\n",
    "    sentence_index_list = []\n",
    "\n",
    "    # Initialize variable for last note start time\n",
    "    last_note_start_time = 0\n",
    "\n",
    "    # Iterate over each downbeat\n",
    "    for i, downbeat in enumerate(downbeats):\n",
    "        # Find the nearest downbeat to the previous downbeat\n",
    "        if downbeat == downbeats[-1]:\n",
    "            next_beat = downbeat + downbeats[-1] - downbeats[-2]\n",
    "        else:\n",
    "            next_beat = downbeats[i+1]\n",
    "        # Get the notes between the previous downbeat and this downbeat\n",
    "        section_notes = []\n",
    "        note_instrument_map = {}\n",
    "        for n, instrument in enumerate(pm.instruments):\n",
    "            for note in instrument.notes:\n",
    "                if next_beat > note.start >= downbeat:\n",
    "                    section_notes.append(note)\n",
    "                    note_instrument_map[note] = (instrument.program, n)\n",
    "        # Sort the notes by start time\n",
    "        section_notes.sort(key=lambda x: x.start)\n",
    "        # Construct the sentence, time since downbeat, duration, and velocity lists\n",
    "        sentence = [note.pitch for note in section_notes]\n",
    "        time_since_downbeat = [note.start - downbeat for note in section_notes]\n",
    "        duration = [note.end - note.start for note in section_notes]\n",
    "        velocity = [note.velocity for note in section_notes]\n",
    "        instrument_program = [note_instrument_map[note][0] for note in section_notes]\n",
    "        track = [note_instrument_map[note][1] for note in section_notes]\n",
    "        time_since_last_note_start = []\n",
    "        for j, note in enumerate(section_notes):\n",
    "            if j == 0:\n",
    "                time_since_last_note_start.append(note.start - last_note_start_time)\n",
    "            else:\n",
    "                time_since_last_note_start.append(note.start - section_notes[j-1].start)\n",
    "        # Append the lists to the overall lists\n",
    "        if len(sentence) > 0:\n",
    "            sentence_list.append(sentence)\n",
    "            time_since_downbeat_list.append(time_since_downbeat)\n",
    "            duration_list.append(duration)\n",
    "            velocity_list.append(velocity)\n",
    "            time_since_last_note_start_list.append(time_since_last_note_start)\n",
    "            instrument_program_list.append(instrument_program)\n",
    "            track_list.append(track)\n",
    "            sentence_index_list.append(i)\n",
    "        # Update last note start time\n",
    "        if len(section_notes) > 0:\n",
    "            last_note_start_time = section_notes[-1].start\n",
    "\n",
    "    # Construct the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Sentence\": sentence_list,\n",
    "        \"TimeSinceLastNoteStart\": time_since_last_note_start_list,\n",
    "        \"TimeSinceDownbeat\": time_since_downbeat_list,\n",
    "        \"Duration\": duration_list,\n",
    "        \"Velocity\": velocity_list,\n",
    "        \"InstrumentProgram\": instrument_program_list,\n",
    "        \"Track\": track_list,\n",
    "        \"SentenceIndex\": sentence_index_list\n",
    "    })\n",
    "    df['Tempo'] = pm.estimate_tempo()\n",
    "    df['MIDI'] = str(uuid.uuid4())[:8]\n",
    "    \n",
    "\n",
    "    return df\n",
    "\n",
    "def df_to_pretty_midi(df:pd.DataFrame, program=0)->pretty_midi.PrettyMIDI:\n",
    "    \"\"\"\n",
    "    Construct a PrettyMIDI object from a DataFrame with columns \"Sentence\", \"Duration\", \"Velocity\", \"TimeSinceLastNoteStart\".\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to construct the PrettyMIDI object from.\n",
    "\n",
    "    Returns:\n",
    "        pretty_midi.PrettyMIDI: A PrettyMIDI object constructed from the DataFrame.\n",
    "    \"\"\"\n",
    "    # Create an empty PrettyMIDI object\n",
    "    pm = pretty_midi.PrettyMIDI()\n",
    "\n",
    "    inst = pretty_midi.Instrument(program=program)\n",
    "    pm.instruments.append(inst)\n",
    "\n",
    "    time = 0\n",
    "    # Iterate over each row in the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        # Get the sentence, time since downbeat, duration, velocity, and time since last note start values\n",
    "        sentence = row[\"Sentence\"]\n",
    "        duration = row[\"Duration\"]\n",
    "        velocity = row[\"Velocity\"]\n",
    "        time_since_last_note_start = row[\"TimeSinceLastNoteStart\"]\n",
    "\n",
    "        for j, pitch in enumerate(sentence):\n",
    "            time += time_since_last_note_start[j]\n",
    "            # Create a note object\n",
    "            note = pretty_midi.Note(velocity=velocity[j], pitch=pitch, start=time, end=time+duration[j])\n",
    "            inst.notes.append(note)\n",
    "\n",
    "    return pm\n",
    "\n",
    "def df_to_src(df:pd.DataFrame, idx=0):\n",
    "    row = df.loc[idx]\n",
    "    TimeSinceLastNoteStart = torch.tensor([PAD_VALUE] + row.at['TimeSinceLastNoteStart'] + [PAD_VALUE]).unsqueeze(-1)\n",
    "    Duration = torch.tensor([PAD_VALUE] + row.at['Duration'] + [PAD_VALUE]).unsqueeze(-1)\n",
    "    Velocity = torch.tensor([PAD_VALUE] + row.at['Velocity'] + [PAD_VALUE]).unsqueeze(-1)/100\n",
    "    extra = torch.cat([TimeSinceLastNoteStart, Duration, Velocity], dim=-1)\n",
    "    sentence = torch.tensor(row.at['Sentence'])\n",
    "    sentence = torch.cat([torch.tensor([BOS_IDX]), sentence, torch.tensor([EOS_IDX])], dim=0)\n",
    "    return (sentence.unsqueeze(0), extra.unsqueeze(0).float())\n",
    "\n",
    "def src_to_df(src):\n",
    "    sentence, extra = src\n",
    "    sentence = sentence[0][1:-1].tolist()\n",
    "    extra = extra[0, 1:-1].transpose(0, 1)\n",
    "    TimeSinceLastNoteStart = extra[0].tolist()\n",
    "    Duration = extra[1].tolist()\n",
    "    Velocity = (extra[2]*100).tolist()\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Sentence\": [sentence],\n",
    "        \"TimeSinceLastNoteStart\": [TimeSinceLastNoteStart],\n",
    "        \"Duration\": [Duration],\n",
    "        \"Velocity\": [Velocity]\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load a midi file and check the data\n",
    "pm = pretty_midi.PrettyMIDI('./Data/MaestroPianoMidi/maestro-v3.0.0/2004/MIDI-Unprocessed_SMF_02_R1_2004_01-05_ORIG_MID--AUDIO_02_R1_2004_05_Track05_wav.midi')\n",
    "# Lets quantize it and plot again\n",
    "pm_quantized = quantize_pretty_midi(pm)\n",
    "print('Original MIDI:')\n",
    "print_midi_info(pm_quantized)\n",
    "plot_piano_roll_with_beats(pm_quantized, xlim=(0, 5))\n",
    "play_pm(pm_quantized)\n",
    "\n",
    "# Lets convert it to dataframe\n",
    "df = pm_to_df(pm_quantized)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Then convert df to src for feeding to the model\n",
    "src = df_to_src(df, 2)\n",
    "# Predict the next midi sentence\n",
    "next_sentence = predict_next_midi_sentence(model, src)\n",
    "# Convert the result to dataframe\n",
    "next_sentence = src_to_df(next_sentence)\n",
    "print('Predicted Next MIDI:')\n",
    "display(next_sentence)\n",
    "# Concatenate it with the original df\n",
    "df_new = pd.concat([df, next_sentence])\n",
    "# Convert to pretty midi object\n",
    "pm_new = df_to_pretty_midi(df_new)\n",
    "pm_new = quantize_pretty_midi(pm_new)\n",
    "print_midi_info(pm_new)\n",
    "# Plot and play\n",
    "plot_piano_roll_with_beats(pm_new, xlim=None)\n",
    "play_pm(pm_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
