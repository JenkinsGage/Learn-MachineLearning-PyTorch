{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import ast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = 0\n",
    "PAD_VALUE = 0.0 \n",
    "\n",
    "PITCH_EMB = 48\n",
    "EXTRA_DIM = 3\n",
    "EXTRA_EMB = 16\n",
    "DROPOUT = 0.2\n",
    "\n",
    "HIDDEN_SIZE_GEN = 256\n",
    "NUM_LAYERS_ENCODER_GEN = 2\n",
    "NUM_LAYERS_GENERATOR_GEN = 2\n",
    "\n",
    "HIDDEN_SIZE_DIS = 64\n",
    "NUM_LAYERS_ENCODER_DIS = 1\n",
    "HIDDEN_LAYERS_DIS = 1\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LEARNING_RATE_GEN = 1e-3\n",
    "LEARNING_RATE_DIS = 1e-4\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "GEN_SAVE_PATH = './Model/MidiGenLSTM.pth'\n",
    "DIS_SAVE_PATH = './Model/MidiDisLSTM.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.1):\n",
    "        super(ResMLP, self).__init__()\n",
    "        self.linear0 = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.linear1(x)\n",
    "        b1 = self.relu(b1)\n",
    "        b1 = self.linear2(b1)\n",
    "        b1 = self.dropout(b1)\n",
    "        b2 = self.linear0(x)\n",
    "        y = b1 + b2\n",
    "        y = self.norm(y)\n",
    "        return y\n",
    "\n",
    "class MidiGenLSTM(nn.Module):\n",
    "    def __init__(self, pitch_emb = 48, extra_dim = 3, extra_emb = 16, hidden_size = 256, num_layers_encoder = 4, num_layers_generator = 4):\n",
    "        super(MidiGenLSTM, self).__init__()\n",
    "        self.pitch_emb = pitch_emb\n",
    "        self.extra_emb = extra_emb\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.num_layers_generator = num_layers_generator\n",
    "        \n",
    "        self.pitch_embedding = nn.Embedding(num_embeddings=128, embedding_dim=pitch_emb)\n",
    "        self.extra_embedding = ResMLP(in_dim=extra_dim, hidden_dim=extra_emb, out_dim=extra_emb, dropout=0.0)\n",
    "        self.encoder = nn.LSTM(input_size=pitch_emb+extra_emb, hidden_size=hidden_size, num_layers=num_layers_encoder)\n",
    "\n",
    "        self.generator = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers_generator)\n",
    "        self.generator_pitch = nn.Sequential(nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.LeakyReLU(), nn.Linear(in_features=hidden_size, out_features=128), nn.Softmax(dim=-1))\n",
    "        self.generator_extra = ResMLP(in_dim=hidden_size, hidden_dim=hidden_size, out_dim=extra_dim)\n",
    "\n",
    "    def encode(self, pitch:torch.Tensor, extra:torch.Tensor)->torch.Tensor:\n",
    "        pitch_emb = self.pitch_embedding(pitch)\n",
    "        extra_emb = self.extra_embedding(extra)\n",
    "        memory, _ = self.encoder(torch.cat((pitch_emb, extra_emb), dim=-1))\n",
    "        return torch.mean(memory, dim=0)\n",
    "    \n",
    "    def generate(self, memory:torch.Tensor, len_latent=16, batch_size=8, memory_factor=0.5)->torch.Tensor:\n",
    "        latent = torch.randn(len_latent, batch_size, self.hidden_size, device=memory.device)\n",
    "        memory_factor = max(0, min(1, memory_factor)) # Clamp memory factor to 0-1\n",
    "        x = memory_factor * memory + (1-memory_factor) * latent\n",
    "        out, _ = self.generator(x)\n",
    "        return out \n",
    "\n",
    "    def forward(self, pitch:torch.Tensor, extra:torch.Tensor, len_latent=16, memory_factor=0.5)->torch.Tensor:\n",
    "        memory = self.encode(pitch, extra)\n",
    "        y = self.generate(memory, len_latent, batch_size=pitch.shape[1], memory_factor=memory_factor)\n",
    "        return self.generator_pitch(y), self.generator_extra(y)\n",
    "    \n",
    "class MidiDiscriminator(nn.Module):\n",
    "    def __init__(self, pitch_emb = 48, extra_dim = 3, extra_emb = 16, hidden_size = 256, num_layers_encoder = 2, dropout=0.2, hidden_layers=2):\n",
    "        super(MidiDiscriminator, self).__init__()\n",
    "        self.pitch_embedding = nn.Embedding(num_embeddings=128, embedding_dim=pitch_emb)\n",
    "        self.extra_embedding = ResMLP(in_dim=extra_dim, hidden_dim=extra_emb, out_dim=extra_emb, dropout=0.0)\n",
    "        self.encoder = nn.LSTM(input_size=pitch_emb+extra_emb, hidden_size=hidden_size, num_layers=num_layers_encoder)\n",
    "        self.bn1 = nn.BatchNorm1d(num_features=hidden_size)\n",
    "\n",
    "        res_mlp_layers = [ResMLP(in_dim=hidden_size, hidden_dim=hidden_size, out_dim=hidden_size, dropout=dropout) for _ in range(hidden_layers)]\n",
    "        self.res_mlp = nn.Sequential(*res_mlp_layers)\n",
    "        self.classifier = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pitch:torch.Tensor, extra:torch.Tensor)->torch.Tensor:\n",
    "        x = torch.cat((self.pitch_embedding(pitch), self.extra_embedding(extra)), dim=-1)\n",
    "        x, _ = self.encoder(x)\n",
    "        x = self.bn1(x.permute(1, 2, 0)).permute(2, 0, 1)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.res_mlp(x)\n",
    "        x = self.classifier(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = MidiGenLSTM(pitch_emb=PITCH_EMB, extra_dim=EXTRA_DIM, extra_emb=EXTRA_EMB, hidden_size=HIDDEN_SIZE_GEN, num_layers_encoder=NUM_LAYERS_ENCODER_GEN, num_layers_generator=NUM_LAYERS_GENERATOR_GEN)\n",
    "print(generator)\n",
    "print(f'Generator Num Params: {sum(p.numel() for p in generator.parameters() if p.requires_grad)/1e6:.2f} M')\n",
    "generator.to(DEVICE)\n",
    "\n",
    "for name, param in generator.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.numel()/1e3:.1f}K')\n",
    "print('-'*64)\n",
    "discriminator = MidiDiscriminator(pitch_emb=PITCH_EMB, extra_dim=EXTRA_DIM, extra_emb=EXTRA_EMB, hidden_size=HIDDEN_SIZE_DIS, num_layers_encoder=NUM_LAYERS_ENCODER_DIS, dropout=DROPOUT, hidden_layers=HIDDEN_LAYERS_DIS)\n",
    "print(discriminator)\n",
    "print(f'Discriminator Num Params: {sum(p.numel() for p in discriminator.parameters() if p.requires_grad)/1e6:.2f} M')\n",
    "discriminator.to(DEVICE)\n",
    "for name, param in discriminator.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.numel()/1e3:.1f}K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MidiDataset(Dataset):\n",
    "    def __init__(self, csv_path='./Data/MidiDataset.csv'):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - 2\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        current_row = self.data.loc[idx]\n",
    "        next_row = self.data.loc[idx+1]\n",
    "\n",
    "        return (\n",
    "            torch.tensor(ast.literal_eval(current_row.at['Sentence'])),\n",
    "            torch.tensor([ast.literal_eval(current_row.at['TimeSinceLastNoteStart']), ast.literal_eval(current_row.at['Duration']), ast.literal_eval(current_row.at['Velocity'])], dtype=torch.float32).transpose(0,1),\n",
    "            torch.tensor(ast.literal_eval(next_row.at['Sentence'])),\n",
    "            torch.tensor([ast.literal_eval(next_row.at['TimeSinceLastNoteStart']), ast.literal_eval(next_row.at['Duration']), ast.literal_eval(next_row.at['Velocity'])], dtype=torch.float32).transpose(0,1)\n",
    "        )\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentence_input, extra_input, sentence_tgt, extra_tgt = zip(*batch)\n",
    "    padded_sentence_input = pad_sequence(sentence_input, padding_value=PAD_IDX)\n",
    "    padded_extra_input = pad_sequence(extra_input, padding_value=PAD_VALUE)\n",
    "    padded_sentence_tgt = pad_sequence(sentence_tgt, padding_value=PAD_IDX)\n",
    "    padded_extra_tgt = pad_sequence(extra_tgt, padding_value=PAD_VALUE)\n",
    "\n",
    "    return padded_sentence_input, padded_extra_input, padded_sentence_tgt, padded_extra_tgt\n",
    "\n",
    "dataset = MidiDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerD = torch.optim.Adam(discriminator.parameters(), lr=LEARNING_RATE_DIS)\n",
    "optimizerG = torch.optim.Adam(generator.parameters(), lr=LEARNING_RATE_GEN)\n",
    "\n",
    "def train_batch(sentence_input, extra_input, sentence_tgt, extra_tgt, optimizerD, optimizerG):\n",
    "    discriminator.zero_grad()\n",
    "    label = torch.full((sentence_tgt.shape[1],), 1., dtype=torch.float).to(DEVICE)\n",
    "    output = discriminator(sentence_tgt, extra_tgt).view(-1)\n",
    "    errD_real = criterion(output, label)\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "    random_memory_factor = torch.normal(0.5, 0.25, size=(1,)).item() # Random factor that controls the noise we apply to the input sentence\n",
    "    sentence_generated, extra_generated = generator(sentence_input, extra_input, len_latent=sentence_tgt.shape[0], memory_factor=random_memory_factor)\n",
    "    sentence_generated = torch.argmax(sentence_generated, dim=-1)\n",
    "\n",
    "    label.fill_(0.)\n",
    "    output = discriminator(sentence_generated.detach(), extra_generated.detach()).view(-1)\n",
    "    errD_fake = criterion(output, label)\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "    errD = errD_real + errD_fake\n",
    "    optimizerD.step()\n",
    "\n",
    "    generator.zero_grad()\n",
    "    label.fill_(1.)\n",
    "    output = discriminator(sentence_generated, extra_generated).view(-1)\n",
    "    errG = criterion(output, label)\n",
    "    errG.backward()\n",
    "    D_G_z2 = output.mean().item()\n",
    "    optimizerG.step()\n",
    "\n",
    "    return errD.item(), errG.item(), D_x, D_G_z1, D_G_z2\n",
    "\n",
    "def train(epochs, writer:SummaryWriter, gdt=0):\n",
    "    discriminator.train()\n",
    "    generator.train()\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for sentence_input, extra_input, sentence_tgt, extra_tgt in tqdm(dataloader, desc=f'Epoch {epoch}'):\n",
    "            sentence_input, extra_input, sentence_tgt, extra_tgt = sentence_input.to(DEVICE), extra_input.to(DEVICE), sentence_tgt.to(DEVICE), extra_tgt.to(DEVICE)\n",
    "            errD, errG, D_x, D_G_z1, D_G_z2 = train_batch(sentence_input, extra_input, sentence_tgt, extra_tgt, optimizerD, optimizerG)\n",
    "            gdt += 1\n",
    "            writer.add_scalar('D_x', D_x, gdt)\n",
    "            writer.add_scalar('D_G_z1', D_G_z1, gdt)\n",
    "            writer.add_scalar('D_G_z2', D_G_z2, gdt)\n",
    "            writer.add_scalar('errD', errD, gdt)\n",
    "            writer.add_scalar('errG', errG, gdt)\n",
    "        torch.save(generator.state_dict(), GEN_SAVE_PATH)\n",
    "        torch.save(discriminator.state_dict(), DIS_SAVE_PATH)\n",
    "\n",
    "tensorboard = SummaryWriter()\n",
    "train(NUM_EPOCHS, tensorboard)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
